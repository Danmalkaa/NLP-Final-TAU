# -*- coding: utf-8 -*-
"""Copy of BERT_FOR_YUVI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hPiJllav1iI-wDCmkwAlrmDgbX60RVSU

To begin copy this notebook to your own drive:

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJAAAAA0CAIAAADqqSYXAAAHL0lEQVR4Ae2b709SXxzH+1e+j3jmM5/5CCrxAhE5moMFYxNatDucYKvxo7BfsuVchaVN25xh3eniG40HsCyK5o8lspyOdLJwMrdCKdcuInB3vpOjJ74i7C5/dNzOecLnns/nfM77fF733B8b9wSo1Jb7FYKm4XUAFh9RAkMAhq0PawVNz74DANan+2mqtkYgENRS9LO5bNG/HGjXiop9IsOj8a2w6fa6Wus4mmN9vJ2qEdRoh7d8vNp0e10NHdoJzW4dFkd/D7UragX/CGpEBosWzlE613I/JTD4iqqWhw11AkFd+zRUY9lSE6BrRK5FmHXaWldb7AQAbC16Z1jZWnZE8PotVg0K2IrPLvqsTXU1AkFNXZPVtwjLBbJzz2jFVhFrRVpXCNbkt/LvIZdBVCvYHrMMpz3Ba3YShE0FCDBsUPATQoDxqxM2UQQYNij4CSHA+NUJmygCDBsU/IQQYPzqhE0UAYYNCn5CCDB+dcImigDDBgU/IScWSDtWFSA7jN+JjU0UAYYNCn5CCDB+dcImigDDBgU/IQQYvzphE/Ub2MLCAjaqiJCKFSDAKpYGTwcBhieXiqoIsIqlwdNBgOHJpaKqgwTm9Xr/LbZIJFJxQuLYXwUOBhjLsrFYrK+vT1dsRqORZdlKwtbX110ul1wuP3PmjNVqXVlZqRT5B/2BQODLly98BjIMIyw2iUTS1tY2Pz9fPoplWaVS+e3bt3LX3+o5GGAej8disbAsazQaITOPx7PnkjiOMxqNdrt9ZWXl58+fbrdbq9Xmcrk9g/+g02w2+3w+PgMZhmlraysUCul02uv1ymSyxcXtfyrC4RzHAQCi0Sg0+OQ8gpgDALa0tAQhBYoN2jqdbmlpqXwBU1NTMpkM7T+O4xobGz9//gwACIVCGo1GKpVevXoVntTJZFIikXR2dqqLLRwOAwCsVuuTJ09g5u7u7lu3bqFZ1Gr1qVOnKIrq6uoCAMTjcZqmKYoyGAxwChQJAGAY5sqVK6int7fXarUCAPx+v16vN5lMzc3NmUxGKBSura3dvXu3v78fBjMMY7PZAABfv36F+U0m0/Ly9h89UcJDMg4AWEdHB4TkcDgAABaLBR52dHSUi37+/Hlra2t5//z8PEVR09PT2Wy2r6/PYDBwHJdMJoVC4fv37wEAU1NTDQ0N6XQ6GAxeuHABZlCr1R8+fCjNhnbYxsaGUqlkGGZzc/Pjx49SqTSVSpVG7gIWjUZlMhkEJhaLP336lM1mEbDJyUmtVguH0zQ9Ojq6ubnZ1NQ0MjKSy+WGhoYuXbpUmvzw7P0Ci0QiaEvBZ41YLNax08o32dOnT69fv16+Hrfb7XK5YD/HcefOnYvFYhAYCr58+XIwGGRZtr6+Ph6PJxIJiqKy2e2/PcMwBCwcDqvVajTW4XC8ePECHZbvsEQiIRQKOY7z+/0tLS0wEgErFApnz55NJBI/fvygKCqTyUxMTKhUKhiWz+dPnz69urpamv+Q7P0CK91PqVTqy/9bObCRkRGz2Vy+GKfTOTg4iPovXrwYCoV2AbPb7bDoNpttYGBgaGjI6XSiIdBAwF69emWxWJD38ePH9+/fR4flwCKRCNph5cAAAJ2dnYODg36//8aNGwAAn88nEonqd9rJkyf3fGwpnfFA7P0C83q9cIelUikED+25QGD7IwqkNRqNyuXyjY0N2MNxnFqtnp2d7enpuXfvHupUKBQzMzO7gOn1+mAwCAB48+ZNc3MzTdNv375FmaGBgI2Pj2s0GuR1OBylJ0Q5sO7u7mvXrsFL4p7AotGoXq+3Wq2h0Na3GWNjYwaDAeU/MmO/wOCTocfjQeQQrdITvHQ9NE3fvn17dXX1169fvb29Wq02n8/H43GpVDozM5PL5QYGBjQaDbqHvX79OpfLjY6OisXidDoNAMhkMmKxuKGhIZPJlGYGANhstp6enlwuB+8xL1++zOfzk5OTFEUlk8nSYHQPS6fTw8PDFEXB94E9L4kAAI7jlEqlXC6HF+FMJnP+/Hmfz1coFBKJRFdX19E8TO4XGAAgEomwLPvgwYOdO9f2bywWKy0QstfW1pxOJ0VRcrnc4XCg97BwOKzT6SiKam1thQ9dcIc9fPhQoVCoVCr4lAjzOBwOu92OciJjbGxMJpPB22EikWhpaZFIJDqdbmJiAsVAo/Q9zGw2z87Owv5KwAAAbrf75s2bKE88HjeZTBKJRKVSwa2PXIdnHACwwxO365JYOtGdO3eOrEal8/51+/gBy+fzc3NzjY2N6GXurxfxKAUcP2AMw8jl8nfv3h1lmfCZC2tg+JQJHyUEGD4seCkhwHiVCZ+g38Dw0USUVKkAAValODi6CDAcqVTRRIBVKQ6OLgIMRypVNJHvw47V12ELC2SHVTmbcXQRYDhSqaKJAKtSHBxdBBiOVKpoIsCqFAdHFwGGI5UqmgiwKsXB0UWA4Uiliqb/AFB0Xp6BwyJDAAAAAElFTkSuQmCC)


### Submission Instructions:
1. **Restart the kernel** (in the menubar, select Runtime$\rightarrow$Restart runtime)
2. **Download the notebook** (in the menubar, select File$\rightarrow$Download .ipynb)
3. **Upload the downloaded notebook (.ipynb file) to your repository**.


Make sure you fill in any place that says `YOUR CODE HERE`, and that no tests fail.  

Note: To use a GPU, do the following: Runtime$\rightarrow$Change runtime type$\rightarrow$ GPU
"""
import datasets as datasets
import gdown
import nltk
gdown.download('https://drive.google.com/uc?export=download&id=1PFOG06NEsTL6VieKQjMk1oNzyzcUtiWn', 'glove.npy', quiet=False)
gdown.download('https://drive.google.com/uc?export=download&id=1-3SxpirQjmX-RCRyRjKdP2L7G_tNgp00', 'vocab.json', quiet=False)

nltk.download('punkt')

!pip install datasets
!pip install transformers

import transformers
from transformers import BertTokenizer, BertModel
import torch
import matplotlib.pyplot as plt
from scipy.spatial.distance import cosine

def get_hidden_state(bert_model, tokenizer, sentence, print_words_ids=False):
  marked_text = "[CLS] " + sentence + " [SEP]"

  encoded_input = tokenizer(marked_text, return_tensors='pt')

  # Split the sentence into tokens.
  tokenized_text = tokenizer.tokenize(marked_text)
  # Map the token strings to their vocabulary indeces.
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
  if print_words_ids:
    # Display the words with their indeces.
    for tup in zip(tokenized_text, indexed_tokens):
      print('{:<12} {:>6,}'.format(tup[0], tup[1]))
    # Output
  with torch.no_grad():
    output = model(**encoded_input)
    hidden_states = output[2]
  return hidden_states

# confirm similarity
from scipy.spatial.distance import cosine

def sentence_similarity(sent_sim1,sent_sim2,sent_diff):
  # Calculate the cosine similarity between the word bank 
  # in "bank robber" vs "river bank" (different meanings).
  diff_sentence = 1 - cosine(sent_sim1, sent_diff)

  # Calculate the cosine similarity between the word bank
  # in "bank robber" vs "bank vault" (same meaning).
  similar_sentence = 1 - cosine(sent_sim1, sent_sim2)

  print('Vector similarity for  *similar*  meanings:  %.2f' % similar_sentence)
  print('Vector similarity for *different* meanings:  %.2f' % diff_sentence)

import transformers
from transformers import BertTokenizer, BertModel
import torch
import matplotlib.pyplot as plt

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained("bert-base-uncased",output_hidden_states = True).eval()
text = "Replace me by any text you'd like."
text2 = "switch me with whatever text you would want."
text3 = "Replace me with him in a love text."

# Add the special tokens.
marked_text = "[CLS] " + text + " [SEP]"

encoded_input = tokenizer(marked_text, return_tensors='pt')

# Split the sentence into tokens.
tokenized_text = tokenizer.tokenize(marked_text)
# Map the token strings to their vocabulary indeces.
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
# Display the words with their indeces.
for tup in zip(tokenized_text, indexed_tokens):
    print('{:<12} {:>6,}'.format(tup[0], tup[1]))

# Output
with torch.no_grad():
  output = model(**encoded_input)
  hidden_states = output[2]

# calc hidden for other 2 sentences
hidden_states2 = get_hidden_state(model,tokenizer,text2)
hidden_states3 = get_hidden_state(model,tokenizer,text3)

hidden_states2 = get_hidden_state(model,tokenizer,text2)# For the 5th token in our sentence, select its feature values from layer 0.
token_i = 5
layer_i = 0
batch_i = 0
vec = hidden_states[layer_i][batch_i][token_i]
# print(hidden_states)

# Plot the values as a histogram to show their distribution.
plt.figure(figsize=(10,10))
plt.hist(vec, bins=200)
plt.show()

# Concatenate the tensors for all layers. We use `stack` here to
# create a new dimension in the tensor.
token_embeddings = torch.stack(hidden_states, dim=0)

print(token_embeddings.size())
token_embeddings = torch.squeeze(token_embeddings, dim=1)
token_embeddings = token_embeddings.permute(1,0,2)
print(token_embeddings.size())

# printing word index in sentence
for i, token_str in enumerate(tokenized_text):
  print (i, token_str)

# `hidden_states` has shape [13 x 1 x 22 x 768]

# `token_vecs` is a tensor with shape [22 x 768]
token_vecs = hidden_states[-1][0]
token_vecs2 = hidden_states2[-1][0]
token_vecs3 = hidden_states3[-1][0]

# Calculate the average of all 22 token vectors.
sentence_embedding = torch.mean(token_vecs, dim=0)
sentence_embedding2 = torch.mean(token_vecs2, dim=0)
sentence_embedding3 = torch.mean(token_vecs3, dim=0)

# confirm similarity
from scipy.spatial.distance import cosine

# Calculate the cosine similarity between the word bank 
# in "bank robber" vs "river bank" (different meanings).
diff_sentence = 1 - cosine(sentence_embedding, sentence_embedding3)

# Calculate the cosine similarity between the word bank
# in "bank robber" vs "bank vault" (same meaning).
similar_sentence = 1 - cosine(sentence_embedding, sentence_embedding2)

print('Vector similarity for  *similar*  meanings:  %.2f' % similar_sentence)
print('Vector similarity for *different* meanings:  %.2f' % diff_sentence)

print(model)

embeddings = model.get_input_embeddings()
print(embeddings)

# (10): BertLayer(
      #   (attention): BertAttention(
      #     (self): BertSelfAttention(
      #       (query): Linear(in_features=768, out_features=768, bias=True)
      #       (key): Linear(in_features=768, out_features=768, bias=True)
      #       (value): Linear(in_features=768, out_features=768, bias=True)
      #       (dropout): Dropout(p=0.1, inplace=False)
      #     )
      #     (output): BertSelfOutput(
      #       (dense): Linear(in_features=768, out_features=768, bias=True)
      #       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      #       (dropout): Dropout(p=0.1, inplace=False)
      #     )
      #   )
      #   (intermediate): BertIntermediate(
      #     (dense): Linear(in_features=768, out_features=3072, bias=True)
      #   )
      #   (output): BertOutput(
      #     (dense): Linear(in_features=3072, out_features=768, bias=True)
      #     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      #     (dropout): Dropout(p=0.1, inplace=False)
      #   )

# model

# dir(model)

for name,x in model.named_parameters():
  # print()
  if "intermediate.dense" in name  or "output.dense" in name:
    # x = x._set(0)
    print(name)

  # break

from sklearn.linear_model import OrthogonalMatchingPursuit

st = model.state_dict()
a = st["encoder.layer.0.attention.output.dense.weight"]
print(a.shape)

# How to replace the model's weights
st = model.state_dict()
st["encoder.layer.1.intermediate.dense.weight"]  = st["encoder.layer.1.intermediate.dense.weight"]*0
model.load_state_dict(st)
print(model.state_dict()["encoder.layer.1.intermediate.dense.weight"])

emb = model.get_input_embeddings()
X = emb.weight.cpu().detach().numpy()
# y= st["encoder.layer.2.intermediate.dense.weight"].cpu().numpy()
y= st["encoder.layer.0.attention.output.dense.weight"].cpu().numpy()

# import sklearn
# from sklearn.linear_model import OrthogonalMatchingPursuit
# print(X.shape)
# print(y.shape)
# mp = OrthogonalMatchingPursuit(normalize=False).fit(X.T, y.T)

# mp.score(X.T,y.T)

# j = mp.predict(X.T)
# print(j.dtype)

# How to replace the model's weights
# print(model.state_dict()["encoder.layer.0.attention.output.dense.weight"])
# st = model.state_dict()
# st["encoder.layer.0.attention.output.dense.weight"]  = torch.Tensor(j.T)
# model.load_state_dict(st)
# print(model.state_dict()["encoder.layer.0.attention.output.dense.weight"])

# calc hidden for sentences with the model - new weights 
hidden_states = get_hidden_state(model,tokenizer,text)
hidden_states2 = get_hidden_state(model,tokenizer,text2)
hidden_states3 = get_hidden_state(model,tokenizer,text3)


for i in range(13):
  print(f"Layer {i}")
  # # `hidden_states` has shape [13 x 1 x 22 x 768]
  # `token_vecs` is a tensor with shape [22 x 768]

  # take the i'th layer 
  token_vecs = hidden_states[i][0]
  token_vecs2 = hidden_states2[i][0]
  token_vecs3 = hidden_states3[i][0]

  # Calculate the average of all 22 token vectors. - a sentence represntation
  sentence_embedding = torch.mean(token_vecs, dim=0)
  sentence_embedding2 = torch.mean(token_vecs2, dim=0)
  sentence_embedding3 = torch.mean(token_vecs3, dim=0)

  sentence_similarity(sentence_embedding,sentence_embedding2,sentence_embedding3) # Calculate based on cosine similarity

# Download wikitext dataset
from datasets import list_datasets, load_dataset, list_metrics, load_metric

wikitext_dataset = load_dataset('wikitext', 'wikitext-103-v1')

# print(wikitext_dataset)
import gc
gc.collect()
torch.cuda.empty_cache()
device = "cuda" if torch.cuda.is_available() else "cpu"
batch_sentences = wikitext_dataset['test'][4000:5500]['text']
# batch_sentences = [data['text'] for i,data in enumerate(wikitext_dataset['train'][1:1500]]
model.to(device)
output_list = [0]*1500
with torch.no_grad():
  for i,sent in enumerate(batch_sentences):
    encoded_input = tokenizer(sent,return_tensors='pt').to(device)
    output = model(**encoded_input)
    output_list[i] = output[2]

output_list[i][0].size()